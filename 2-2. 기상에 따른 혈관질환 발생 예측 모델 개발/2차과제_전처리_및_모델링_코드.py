# -*- coding: utf-8 -*-
"""2차과제_전처리_및_모델링_코드.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s6i4ok-L5_fKP54HNyzJazXvA4mIkC6i

## 예보데이터 결합
- 지역별/변수별 나누어져 있던 데이터를 연도별로 결합
"""

def add_mean(path, feature):

  #df = pd.read_csv(path, encoding='cp949')
    df = path

  # 첫행에 ' Start : 20120101 ' 추가(패턴을 맞춰주기위해)
    #new = {'format..day' : [' Start : 20130101 '], 'hour' : [np.nan], 'forecast' : [np.nan], 'value  location:65_139 Start : 20130101 ' : [np.nan]}
    
    new = {' format: day' : [' Start : 20160101 '], 'hour' : [np.nan], 'forecast' : [np.nan], 'value  location:65_139 Start : 20160101 ' : [np.nan]}
    #new = {' format: day' : [' Start : 20160101 '], 'hour' : [np.nan], 'forecast' : [np.nan], 'value  location:75_139 Start : 20160101 ' : [np.nan]}
    new = pd.DataFrame(new)
    df1 = pd.concat([new, df])
    df1.reset_index(drop=True, inplace=True)
    
    #if df[df.columns[1]].dtypes != 'float64' or df[df.columns[2]].dtypes != 'float64' or df[df.columns[3]].dtypes != 'float64':
    #    df[df.columns[1]] = pd.to_numeric(df[df.columns[1]], errors='coerce')
    #    df[df.columns[2]] = pd.to_numeric(df[df.columns[2]], errors='coerce')
        #df[df.columns[3]] = pd.to_numeric(df[df.columns[3]], errors='coerce')

  # 컬럼명 바꾸기
    #df2 = df1.rename(columns={'format..day':'yymmdd'})
    df2 = df1.rename(columns={' format: day':'yymmdd'})
                              # , 'value  location:65_139 Start : 20120101 ':feature})

  # ' Start : * ' 인덱스 탐색
    idx = df2.index[df2['yymmdd'].str.contains('Start', case=False, na=False)].tolist()

  # 2012____ 리스트에 저장
    dates = []
    for i in range(len(idx)):
        date = df2.iloc[idx[i], 0].split(': ')[1].strip()
        dates.append(date[:-2])
    print(dates)

  # 'yymmdd'(몇일) 리스트에 저장
    days = []
    for i in range(len(idx)):
        if idx[i] == idx[-1]:
            day = list(df2['yymmdd'][idx[i]+1:].map('{}'.format))
    
        else:
            day = list(df2['yymmdd'][idx[i]+1:idx[i+1]].map('{}'.format))
        days.append(day)
    print(days)

  # 'yymmdd':  1,2,3...->20120101, 20120102, 20120103... 바꾸기
    df3 = df2
    num = 0

    for date, day in zip(dates, days):
        j=0
        start = idx[num]+1
        if idx[num] == idx[-1]:
            finish = len(df2)
        else:
            finish = idx[num+1]

        for i in range(start, finish):
            df3.iloc[i, 0] = f'{date}{day[j].strip().zfill(2)}'
            j+=1
        num+=1
  
    idx = df3.index[df3['yymmdd'].str.contains('Start', case=False, na=False)].tolist()
    df3 = df3.drop(idx, axis = 0)
    #df3 = df3.apply(pd.to_numeric)

  # ['yymmdd','hour',]으로 묶고 feature의 평균 구하기
    df4 = df3.groupby(['yymmdd','hour'])[feature].mean()
  
  # ['yymmdd']으로 묶고 feature의 평균 구하기
    df5 = df3.groupby(['yymmdd'])[feature].mean()
  
    return pd.DataFrame(df4), pd.DataFrame(df5)

#area_list = ['강원']
area_list = ['광주', '대구', '대전', '부산', '서울', '세종', '울산', '인천', '전남', '전북', '제주', '충남', '충북']
# '광주', '대구', '대전', '부산', '서울', '세종', '울산', 
#'강원', '경기', '경남', '경북',
#area_list = ['광주', '대구', '대전', '부산', '서울', '세종', '울산', '인천', '전남', '전북', '제주', '충남', '충북']

#data_list = ['3시간기온', '6시간강수량'
#             '6시간적설', '12시간강수량', '12시간신적설', '강수형태', '강수확률', 
#             '습도', '일최고기온', '일최저기온', '풍속', '풍향', '하늘상태']

#data_list = ['적설', '강수량']
data_list = ['하늘상태']

for i in range(0, len(area_list)):
    for j in range(0, len(data_list)):
        file_names = glob("C:\\Users\\82109\\Desktop\\2차_과제\\2016\\%s\\*%s*\\*.csv" %(area_list[i], data_list[j])) #폴더 내의 모든 csv파일 목록을 불러온다
        total = pd.DataFrame() #빈 데이터프레임 하나를 생성한다

        for file_name in file_names:
            temp = pd.read_csv(file_name) #csv파일을 하나씩 열어 임시 데이터프레임으로 생성한다
            temp.rename(columns = lambda x: x.split('_')[-1], inplace = True)
            total = pd.concat([total, temp], axis=1) #전체 데이터프레임에 추가하여 넣는다
        #48191    
        total_1 = total.iloc[:49590, :2]
        total_2 = total.iloc[:49590, :]
        
        total_2 = total_2.drop([' format: day', 'hour', 'forecast'], axis=1, inplace=False)
        #total_2 = total.drop(['format..day', 'hour', 'forecast'], axis=1, inplace=False)
        
        #total_2 = total.drop(total.iloc[:, :3])
        data = pd.concat([total_1, total_2], axis=1)
        #data = data.apply(pd.to_numeric)
        
        data_1 = data.iloc[:49590, 2:]
        
        mask = data_1.isin(['Start'])
        data_1 = data_1[~mask]
        mask_2 = data_1.isin(['value  location:59_75 Start : 20160101 '])
        data_1 = data_1[~mask_2]
        mask_1 = data_1.isin(['201612format: day'])
        data_1 = data_1[~mask_1]
        data_1 = data_1.apply(pd.to_numeric, errors='ignore')
        data_1 = data_1.astype(float)
        data_mean = data_1.mean(axis=1)
        
        total_data = pd.concat([data.iloc[:, :2], data_mean], axis=1)
        total_data = total_data.rename(columns = {0:'%s'%(data_list[j])})
        
        total_data = total_data[total_data['hour'] != 200.0]
        total_data = total_data[total_data['hour'] != 500.0]
        total_data = total_data[total_data['hour'] != 2000.0]
        total_data = total_data[total_data['hour'] != 2300.0]
        
        feature = '%s'%(data_list[j])
        final1, final2 = add_mean(total_data, feature)
        
        final2['area'] = '%s'%(area_list[i])

        final2.to_csv("C:\\Users\\82109\\Desktop\\2차_데이터_결합\\2016\\%s_%s.csv"%(area_list[i], data_list[j]), encoding='cp949')

def add_mean(path, feature):

  #df = pd.read_csv(path, encoding='cp949')
    df = path

  # 첫행에 ' Start : 20120101 ' 추가(패턴을 맞춰주기위해)
    new = {' format: day' : [' Start : 20150101 '], 'hour' : [np.nan], 'forecast' : [np.nan], 'value  location:65_139 Start : 20150101 ' : [np.nan]}
    new = pd.DataFrame(new)
    df1 = pd.concat([new, df])
    df1.reset_index(drop=True, inplace=True)

  # 컬럼명 바꾸기
    df2 = df1.rename(columns={' format: day':'yymmdd'})
                              # , 'value  location:65_139 Start : 20120101 ':feature})

  # ' Start : * ' 인덱스 탐색
    idx = df2.index[df2['yymmdd'].str.contains('Start', na=False)].tolist()

  # 2012____ 리스트에 저장
    dates = []
    for i in range(len(idx)):
        date = df2.iloc[idx[i], 0].split(': ')[1].strip()
        dates.append(date[:-2])
    print(dates)

  # 'yymmdd'(몇일) 리스트에 저장
    days = []
    for i in range(len(idx)):
        if idx[i] == idx[-1]:
            day = list(df2['yymmdd'][idx[i]+1:].map('{}'.format))
    
        else:
            day = list(df2['yymmdd'][idx[i]+1:idx[i+1]].map('{}'.format))
        days.append(day)
    print(days)

  # 'yymmdd':  1,2,3...->20120101, 20120102, 20120103... 바꾸기
    df3 = df2
    num = 0

    for date, day in zip(dates, days):
        j=0
        start = idx[num]+1
        if idx[num] == idx[-1]:
            finish = len(df2)
        else:
            finish = idx[num+1]

        for i in range(start, finish):
            df3.iloc[i, 0] = f'{date}{day[j].strip().zfill(2)}'
            j+=1
        num+=1
  
    idx = df3.index[df3['yymmdd'].str.contains('Start')].tolist()
    df3 = df3.drop(idx, axis = 0)
    df3 = df3.apply(pd.to_numeric)

  # ['yymmdd','hour',]으로 묶고 feature의 평균 구하기
    df4 = df3.groupby(['yymmdd','hour'])[feature].mean()
  
  # ['yymmdd']으로 묶고 feature의 평균 구하기
    df5 = df3.groupby(['yymmdd'])[feature].mean()
  
    return pd.DataFrame(df4), pd.DataFrame(df5)

##############

file_names = glob("C:\\Users\\82109\\Desktop\\2차_과제\\2015\\경북\\습도\\*.csv") #폴더 내의 모든 csv파일 목록을 불러온다
total = pd.DataFrame() #빈 데이터프레임 하나를 생성한다

for file_name in file_names:
    temp = pd.read_csv(file_name) #csv파일을 하나씩 열어 임시 데이터프레임으로 생성한다
    temp.rename(columns = lambda x: x.split('_')[-1], inplace = True)
    total = pd.concat([total, temp], axis=1) #전체 데이터프레임에 추가하여 넣는다
            
total_1 = total.iloc[:, :2]
total_2 = total.drop([' format: day', 'hour', 'forecast'], axis=1, inplace=False)
        #total_2 = total.drop(total.iloc[:, :3])
data = pd.concat([total_1, total_2], axis=1)
        #data = data.apply(pd.to_numeric)

data_1 = data.iloc[:, 2:]
data_mean = data_1.mean(axis=1)
        
total_data = pd.concat([data.iloc[:, :2], data_mean], axis=1)
total_data = total_data.rename(columns = {0:'습도'})
        
total_data = total_data[total_data['hour'] != 200.0]
total_data = total_data[total_data['hour'] != 500.0]
total_data = total_data[total_data['hour'] != 2000.0]
total_data = total_data[total_data['hour'] != 2300.0]
        
feature = '습도'
final1, final2 = add_mean(total_data, feature)
        
final2['area'] = '경북'

final2.to_csv("C:\\Users\\82109\\Desktop\\2차_데이터_결합\\2015\\경북_습도.csv", encoding='cp949')

import pandas as pd
import numpy as np
from glob import glob

hos = pd.read_csv("C:\\Users\\82109\\Downloads\\back_hospital_.csv")

file_names = glob("C:\\Users\\82109\\Desktop\\2차_데이터_결합\\결합\\20*.csv") #폴더 내의 모든 csv파일 목록을 불러온다
total = pd.DataFrame() #빈 데이터프레임 하나를 생성한다

for file_name in file_names:
    temp = pd.read_csv(file_name, encoding='cp949') #csv파일을 하나씩 열어 임시 데이터프레임으로 생성한다
    temp.rename(columns = lambda x: x.split('_')[-1], inplace = True)
    total = pd.concat([total, temp]) #전체 데이터프레임에 추가하여 넣는다
    
total.to_csv('C:\\Users\\82109\\Desktop\\2차_데이터_결합\\결합\\예보.csv', encoding='cp949')

hos.columns

a = pd.merge(hos, total,how='inner',on=['yymmdd','area'])
a

a.to_csv('C:\\Users\\82109\\Desktop\\2차_데이터_결합\\결합\\예보_병원.csv', encoding='cp949')

"""## 예보데이터 결합
- 지역별/변수별 나누어져 있던 데이터를 연도별로 결합
"""

def add_mean(path, feature):

  #df = pd.read_csv(path, encoding='cp949')
    df = path

  # 첫행에 ' Start : 20120101 ' 추가(패턴을 맞춰주기위해)
    #new = {'format..day' : [' Start : 20130101 '], 'hour' : [np.nan], 'forecast' : [np.nan], 'value  location:65_139 Start : 20130101 ' : [np.nan]}
    
    new = {' format: day' : [' Start : 20160101 '], 'hour' : [np.nan], 'forecast' : [np.nan], 'value  location:65_139 Start : 20160101 ' : [np.nan]}
    #new = {' format: day' : [' Start : 20160101 '], 'hour' : [np.nan], 'forecast' : [np.nan], 'value  location:75_139 Start : 20160101 ' : [np.nan]}
    new = pd.DataFrame(new)
    df1 = pd.concat([new, df])
    df1.reset_index(drop=True, inplace=True)
    
    #if df[df.columns[1]].dtypes != 'float64' or df[df.columns[2]].dtypes != 'float64' or df[df.columns[3]].dtypes != 'float64':
    #    df[df.columns[1]] = pd.to_numeric(df[df.columns[1]], errors='coerce')
    #    df[df.columns[2]] = pd.to_numeric(df[df.columns[2]], errors='coerce')
        #df[df.columns[3]] = pd.to_numeric(df[df.columns[3]], errors='coerce')

  # 컬럼명 바꾸기
    #df2 = df1.rename(columns={'format..day':'yymmdd'})
    df2 = df1.rename(columns={' format: day':'yymmdd'})
                              # , 'value  location:65_139 Start : 20120101 ':feature})

  # ' Start : * ' 인덱스 탐색
    idx = df2.index[df2['yymmdd'].str.contains('Start', case=False, na=False)].tolist()

  # 2012____ 리스트에 저장
    dates = []
    for i in range(len(idx)):
        date = df2.iloc[idx[i], 0].split(': ')[1].strip()
        dates.append(date[:-2])
    print(dates)

  # 'yymmdd'(몇일) 리스트에 저장
    days = []
    for i in range(len(idx)):
        if idx[i] == idx[-1]:
            day = list(df2['yymmdd'][idx[i]+1:].map('{}'.format))
    
        else:
            day = list(df2['yymmdd'][idx[i]+1:idx[i+1]].map('{}'.format))
        days.append(day)
    print(days)

  # 'yymmdd':  1,2,3...->20120101, 20120102, 20120103... 바꾸기
    df3 = df2
    num = 0

    for date, day in zip(dates, days):
        j=0
        start = idx[num]+1
        if idx[num] == idx[-1]:
            finish = len(df2)
        else:
            finish = idx[num+1]

        for i in range(start, finish):
            df3.iloc[i, 0] = f'{date}{day[j].strip().zfill(2)}'
            j+=1
        num+=1
  
    idx = df3.index[df3['yymmdd'].str.contains('Start', case=False, na=False)].tolist()
    df3 = df3.drop(idx, axis = 0)
    #df3 = df3.apply(pd.to_numeric)

  # ['yymmdd','hour',]으로 묶고 feature의 평균 구하기
    df4 = df3.groupby(['yymmdd','hour'])[feature].mean()
  
  # ['yymmdd']으로 묶고 feature의 평균 구하기
    df5 = df3.groupby(['yymmdd'])[feature].mean()
  
    return pd.DataFrame(df4), pd.DataFrame(df5)

#area_list = ['강원']
area_list = ['광주', '대구', '대전', '부산', '서울', '세종', '울산', '인천', '전남', '전북', '제주', '충남', '충북']
# '광주', '대구', '대전', '부산', '서울', '세종', '울산', 
#'강원', '경기', '경남', '경북',
#area_list = ['광주', '대구', '대전', '부산', '서울', '세종', '울산', '인천', '전남', '전북', '제주', '충남', '충북']

#data_list = ['3시간기온', '6시간강수량'
#             '6시간적설', '12시간강수량', '12시간신적설', '강수형태', '강수확률', 
#             '습도', '일최고기온', '일최저기온', '풍속', '풍향', '하늘상태']

#data_list = ['적설', '강수량']
data_list = ['하늘상태']

for i in range(0, len(area_list)):
    for j in range(0, len(data_list)):
        file_names = glob("C:\\Users\\82109\\Desktop\\2차_과제\\2016\\%s\\*%s*\\*.csv" %(area_list[i], data_list[j])) #폴더 내의 모든 csv파일 목록을 불러온다
        total = pd.DataFrame() #빈 데이터프레임 하나를 생성한다

        for file_name in file_names:
            temp = pd.read_csv(file_name) #csv파일을 하나씩 열어 임시 데이터프레임으로 생성한다
            temp.rename(columns = lambda x: x.split('_')[-1], inplace = True)
            total = pd.concat([total, temp], axis=1) #전체 데이터프레임에 추가하여 넣는다
        #48191    
        total_1 = total.iloc[:49590, :2]
        total_2 = total.iloc[:49590, :]
        
        total_2 = total_2.drop([' format: day', 'hour', 'forecast'], axis=1, inplace=False)
        #total_2 = total.drop(['format..day', 'hour', 'forecast'], axis=1, inplace=False)
        
        #total_2 = total.drop(total.iloc[:, :3])
        data = pd.concat([total_1, total_2], axis=1)
        #data = data.apply(pd.to_numeric)
        
        data_1 = data.iloc[:49590, 2:]
        
        mask = data_1.isin(['Start'])
        data_1 = data_1[~mask]
        mask_2 = data_1.isin(['value  location:59_75 Start : 20160101 '])
        data_1 = data_1[~mask_2]
        mask_1 = data_1.isin(['201612format: day'])
        data_1 = data_1[~mask_1]
        data_1 = data_1.apply(pd.to_numeric, errors='ignore')
        data_1 = data_1.astype(float)
        data_mean = data_1.mean(axis=1)
        
        total_data = pd.concat([data.iloc[:, :2], data_mean], axis=1)
        total_data = total_data.rename(columns = {0:'%s'%(data_list[j])})
        
        total_data = total_data[total_data['hour'] != 200.0]
        total_data = total_data[total_data['hour'] != 500.0]
        total_data = total_data[total_data['hour'] != 2000.0]
        total_data = total_data[total_data['hour'] != 2300.0]
        
        feature = '%s'%(data_list[j])
        final1, final2 = add_mean(total_data, feature)
        
        final2['area'] = '%s'%(area_list[i])

        final2.to_csv("C:\\Users\\82109\\Desktop\\2차_데이터_결합\\2016\\%s_%s.csv"%(area_list[i], data_list[j]), encoding='cp949')

def add_mean(path, feature):

  #df = pd.read_csv(path, encoding='cp949')
    df = path

  # 첫행에 ' Start : 20120101 ' 추가(패턴을 맞춰주기위해)
    new = {' format: day' : [' Start : 20150101 '], 'hour' : [np.nan], 'forecast' : [np.nan], 'value  location:65_139 Start : 20150101 ' : [np.nan]}
    new = pd.DataFrame(new)
    df1 = pd.concat([new, df])
    df1.reset_index(drop=True, inplace=True)

  # 컬럼명 바꾸기
    df2 = df1.rename(columns={' format: day':'yymmdd'})
                              # , 'value  location:65_139 Start : 20120101 ':feature})

  # ' Start : * ' 인덱스 탐색
    idx = df2.index[df2['yymmdd'].str.contains('Start', na=False)].tolist()

  # 2012____ 리스트에 저장
    dates = []
    for i in range(len(idx)):
        date = df2.iloc[idx[i], 0].split(': ')[1].strip()
        dates.append(date[:-2])
    print(dates)

  # 'yymmdd'(몇일) 리스트에 저장
    days = []
    for i in range(len(idx)):
        if idx[i] == idx[-1]:
            day = list(df2['yymmdd'][idx[i]+1:].map('{}'.format))
    
        else:
            day = list(df2['yymmdd'][idx[i]+1:idx[i+1]].map('{}'.format))
        days.append(day)
    print(days)

  # 'yymmdd':  1,2,3...->20120101, 20120102, 20120103... 바꾸기
    df3 = df2
    num = 0

    for date, day in zip(dates, days):
        j=0
        start = idx[num]+1
        if idx[num] == idx[-1]:
            finish = len(df2)
        else:
            finish = idx[num+1]

        for i in range(start, finish):
            df3.iloc[i, 0] = f'{date}{day[j].strip().zfill(2)}'
            j+=1
        num+=1
  
    idx = df3.index[df3['yymmdd'].str.contains('Start')].tolist()
    df3 = df3.drop(idx, axis = 0)
    df3 = df3.apply(pd.to_numeric)

  # ['yymmdd','hour',]으로 묶고 feature의 평균 구하기
    df4 = df3.groupby(['yymmdd','hour'])[feature].mean()
  
  # ['yymmdd']으로 묶고 feature의 평균 구하기
    df5 = df3.groupby(['yymmdd'])[feature].mean()
  
    return pd.DataFrame(df4), pd.DataFrame(df5)

##############

file_names = glob("C:\\Users\\82109\\Desktop\\2차_과제\\2015\\경북\\습도\\*.csv") #폴더 내의 모든 csv파일 목록을 불러온다
total = pd.DataFrame() #빈 데이터프레임 하나를 생성한다

for file_name in file_names:
    temp = pd.read_csv(file_name) #csv파일을 하나씩 열어 임시 데이터프레임으로 생성한다
    temp.rename(columns = lambda x: x.split('_')[-1], inplace = True)
    total = pd.concat([total, temp], axis=1) #전체 데이터프레임에 추가하여 넣는다
            
total_1 = total.iloc[:, :2]
total_2 = total.drop([' format: day', 'hour', 'forecast'], axis=1, inplace=False)
        #total_2 = total.drop(total.iloc[:, :3])
data = pd.concat([total_1, total_2], axis=1)
        #data = data.apply(pd.to_numeric)

data_1 = data.iloc[:, 2:]
data_mean = data_1.mean(axis=1)
        
total_data = pd.concat([data.iloc[:, :2], data_mean], axis=1)
total_data = total_data.rename(columns = {0:'습도'})
        
total_data = total_data[total_data['hour'] != 200.0]
total_data = total_data[total_data['hour'] != 500.0]
total_data = total_data[total_data['hour'] != 2000.0]
total_data = total_data[total_data['hour'] != 2300.0]
        
feature = '습도'
final1, final2 = add_mean(total_data, feature)
        
final2['area'] = '경북'

final2.to_csv("C:\\Users\\82109\\Desktop\\2차_데이터_결합\\2015\\경북_습도.csv", encoding='cp949')

import pandas as pd
import numpy as np
from glob import glob

hos = pd.read_csv("C:\\Users\\82109\\Downloads\\back_hospital_.csv")

file_names = glob("C:\\Users\\82109\\Desktop\\2차_데이터_결합\\결합\\20*.csv") #폴더 내의 모든 csv파일 목록을 불러온다
total = pd.DataFrame() #빈 데이터프레임 하나를 생성한다

for file_name in file_names:
    temp = pd.read_csv(file_name, encoding='cp949') #csv파일을 하나씩 열어 임시 데이터프레임으로 생성한다
    temp.rename(columns = lambda x: x.split('_')[-1], inplace = True)
    total = pd.concat([total, temp]) #전체 데이터프레임에 추가하여 넣는다
    
total.to_csv('C:\\Users\\82109\\Desktop\\2차_데이터_결합\\결합\\예보.csv', encoding='cp949')

hos.columns

a = pd.merge(hos, total,how='inner',on=['yymmdd','area'])
a

a.to_csv('C:\\Users\\82109\\Desktop\\2차_데이터_결합\\결합\\예보_병원.csv', encoding='cp949')

"""## 종관기상관측 데이터 전처리"""

import pandas as pd
from glob import glob

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/종관기상관측/2012/강원/강원도_강릉_DAY_2012.csv', encoding='cp949')
df

# 필요한 데이터만 추출
df = df[['지점', '일시', '평균기온(°C)', '최저기온(°C)', '최고기온(°C)', '평균 풍속(m/s)', '평균 이슬점온도(°C)', '평균 상대습도(%)', '평균 증기압(hPa)', '1시간 최다일사량(MJ/m2)', '평균 지면온도(°C)']]
df

"""### 1. 이상치 제거
- 0.25 ~ 0.75 범위 사용
"""

df4 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/종관기상관측/2012/경남/경남_거제_DAY_2012.csv', encoding='cp949')
df4 = df4[['지점', '일시', '평균기온(°C)', '최저기온(°C)', '최고기온(°C)', '평균 풍속(m/s)', '평균 이슬점온도(°C)', '평균 상대습도(%)', '평균 증기압(hPa)', '평균 지면온도(°C)']] #, '1시간 최다일사량(MJ/m2)'

level_1q = df4.quantile(0.25)
print(level_1q,"\n")

level_3q = df4.quantile(0.75)
print(level_3q,"\n")
IQR = level_3q - level_1q

rev_range = 1.5  # 제거 범위 조절 변수
dff = df4[(df4 <= level_3q + (rev_range * IQR)) & (df4 >= level_1q - (rev_range * IQR))]
dff = dff.reset_index(drop=True)
dff[['지점', '일시']] = df4[['지점', '일시']]

dff

"""### 2. 결측치 처리"""

dff1 = dff.fillna(dff.interpolate()) # 가장 최근 위아래 평균으로 결측치 대입
dff1

dff1.isnull().sum()

"""### 3. 2012 ~ 2016년, 각 년도의 각 지역으로 통합
- 위의 내용들을 함수로 만들어 한번에 실행되도록 만듬
"""

def remove_add(area, year):
  # 이상치 제거 & 지역 합치기
  file_names = glob(f'/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/종관기상관측/{year}/{area}/*.csv') #폴더 내의 모든 csv파일 목록을 불러온다
  total = pd.DataFrame() #빈 데이터프레임 하나를 생성한다

  for file_name in file_names:
    df4 = pd.read_csv(file_name, encoding='cp949') #csv파일을 하나씩 열어 임시 데이터프레임으로 생성한다
    
    # 필요한 변수만 가져옴
    df4 = df4[['지점', '일시', '평균기온(°C)', '최저기온(°C)', '최고기온(°C)', '평균 풍속(m/s)', '평균 이슬점온도(°C)', '평균 상대습도(%)', '평균 증기압(hPa)', '평균 지면온도(°C)']] #, '1시간 최다일사량(MJ/m2)'

    # 이상치 처리(0.25~0.75)
    level_1q = df4.quantile(0.25)
    level_3q = df4.quantile(0.75)
    IQR = level_3q - level_1q

    rev_range = 1.5  # 제거 범위 조절 변수
    dff = df4[(df4 <= level_3q + (rev_range * IQR)) & (df4 >= level_1q - (rev_range * IQR))]
    dff = dff.reset_index(drop=True)
    dff[['지점', '일시']] = df4[['지점', '일시']]

    total = pd.concat([total, dff])

  
  # 결측치 처리
  total_N = total.fillna(total.interpolate()) # 가장 최근 위아래 평균으로 결측치 대입


  # ['일시'] 컬럼을 기준으로 그룹화 한 후 데이터 평균 구하기
  total_C = total_N.groupby(['일시']).mean()
  total_C = total_C.reset_index()
  total_C = total_C[['지점', '일시', '평균기온(°C)', '최저기온(°C)', '최고기온(°C)', '평균 풍속(m/s)', '평균 이슬점온도(°C)', '평균 상대습도(%)', '평균 증기압(hPa)', '평균 지면온도(°C)']] #, '1시간 최다일사량(MJ/m2)'
    
  total_C['지점'] = area

  return total_C

area_list = ['강원', '경기', '경남', '경북', '광주', '대구', '대전', '부산', '서울', '울산', '인천',
            '전남', '전북', '제주', '충남', '충북']
years = ['2012', '2013', '2014', '2015', '2016']

for year in years:
  for area in area_list:
    total_C = remove_add(area, "2015")
    total_C.to_csv(f'/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/종관기상관측/지역별통합/{year}/{area}_total.csv', index=False, encoding='cp949') # csv 파일로 저장

"""## 황사 데이터 전처리"""

file_names = glob('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/황사/2012/강원/*.csv')
total = pd.DataFrame()

# 강원 지역파일에 있는 모든 .csv파일 concat
for file_name in file_names:
  print(file_name)
  df = pd.read_csv(file_name, encoding='cp949')
  total = pd.concat([total,df])
total

# 각 데이터 프레임의 마지막에 다음해의 첫일이 있음
# 마지막행 => 2013-01-01 00:00

# '시간'컬럼에서 시간(01:00)에 해당하는 부분 제거

splited = total['시간'].str.split(' ', expand=True)
splited.columns = ['일시','시간']
new_df = pd.concat([total, splited], axis=1)
new_df.drop(['시간'], axis=1, inplace=True)
new_df

# 하루 단위로 평균값구하기

new_df1 = new_df.groupby(['일시']).mean()
new_df1 = new_df1.reset_index()
new_df1

# '지점'컬럼의 모든 데이터값을 '강원'으로 바꿈

new_df1['지점'] = '강원'
new_df1 = new_df1[['지점', '일시', '1시간평균 미세먼지농도(㎍/㎥)']]

new_df1 = new_df1.iloc[:-1,:]
new_df1

# 2012 ~ 2016년, 각 년도의 각 지역으로 통합
# 종관기상관측 데이터 전처리와 마찬가지로 위의 내용을 함수로 만들어 한번에 실행시킴

"""## 학습데이터 생성

### 1. 지역 합치기 & 정렬
"""

# 지역 합치기 & 정렬

def add_sort(file_names):
  total = pd.DataFrame()

  # 데이터 아래로 합치기
  for file_name in file_names:
    df = pd.read_csv(file_name, encoding='cp949')
    total = pd.concat([total,df])

  # ['일시','지점'] 우선순위로 정렬
  total=total.sort_values(by=[df.columns[1],df.columns[0]])

  # ['일시'] 컬럼 데이터 변경: 2012-01-01 -> 20120101
  # 데이터 타입 변경
  total['일시']= total['일시'].str.replace(pat=r'-',repl=r'',regex=True)
  total['일시'] = pd.to_numeric(total['일시'], errors='coerce')

  # 컬럼명 변경: '일시' -> 'yymmdd' / '지점' -> 'area'
  total = total.rename(columns={'일시':'yyyymmdd', '지점':'area'})

  return total

# 황사데이터 모든 지역 통합

years = ['2012', '2013', '2014', '2015', '2016']
total_C = pd.DataFrame()

for year in years:
  file_names = glob(f'/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/황사/지역별통합/{year}/*')
  total_C = pd.concat([total_C, add_sort(file_names)])

total_C.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/황사/total.csv', index=False, encoding='cp949')

total_C

# 관측데이터 모든 지역 통합

years = ['2012', '2013', '2014', '2015', '2016']
total_C = pd.DataFrame()

for year in years:
  file_names = glob(f'/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/종관기상관측/지역별통합/{year}/*')

  total_C = pd.concat([total_C, add_sort(file_names)])

total_C.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/종관기상관측/total.csv', index=False, encoding='cp949')

total_C

"""## 백병원과 변수 합치기"""

# 백병원 + 황사

hospital = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/back_hospital_.csv', index_col=0)

# hospital = hospital.drop(['Unnamed: 0_x', 'Unnamed: 0_y'], axis=1, inplace=False)
hospital.columns = ['yyyymmdd', 'area', hospital.columns[2], hospital.columns[3]]

total_C = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/황사/total.csv', encoding='cp949')

# 백병원데이터의 'yyyymmdd', 'area' 컬럼을 기준으로 황사 데이터와 결합
total = pd.merge(hospital, total_C, on=["yyyymmdd", "area"], how="left")
total_final = total.drop(['back_hospital.sex', 'back_hospital.frequency'], axis=1, inplace=False)
total_final['back_hospital.sex'] = total['back_hospital.sex']
total_final['back_hospital.frequency'] = total['back_hospital.frequency']

total_final.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/모델링데이터/hospital_feature.csv', index=False, encoding='cp949')

total_final

# [백병원 + 황사] + 관측
hospital = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/모델링데이터/hospital_feature.csv', encoding='cp949')

total_C = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/종관기상관측/total.csv', encoding='cp949')

# [백병원+황사]데이터의 'yyyymmdd', 'area' 컬럼을 기준으로 관측 데이터와 결합
total = pd.merge(hospital, total_C, on=["yyyymmdd", "area"], how="left")
total_final = total.drop(['back_hospital.sex', 'back_hospital.frequency'], axis=1, inplace=False)
total_final['back_hospital.sex'] = total['back_hospital.sex']
total_final['back_hospital.frequency'] = total['back_hospital.frequency']

total_final.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/모델링데이터/hospital_feature1.csv', index=False, encoding='cp949')

total_final

# [백병원 + 황사 + 관측] + 예보
hospital = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/모델링데이터/hospital_feature1.csv', encoding='cp949')

total_C = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/예보데이터/예보.csv', index_col=0, encoding='cp949')
total_C = total_C.drop(['Unnamed: 0.1'], axis=1, inplace=False)
total_C = total_C.rename(columns={'yymmdd':'yyyymmdd'})

# [백병원+황사+관측]데이터의 'yyyymmdd', 'area' 컬럼을 기준으로 예보 데이터와 결합
total = pd.merge(hospital, total_C, on=["yyyymmdd", "area"], how="left")
total_final = total.drop(['back_hospital.sex', 'back_hospital.frequency'], axis=1, inplace=False)
total_final['back_hospital.sex'] = total['back_hospital.sex']
total_final['back_hospital.frequency'] = total['back_hospital.frequency']

total_final.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/모델링데이터/hospital_feature2.csv', index=False, encoding='cp949')

# 검증데이터셋에 맞춰 컬럼명 변경
# 'back_hospital.sex' -> 'sex', 'back_hospital.frequency' -> 'frequency'

all_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/모델링데이터/hospital_feature2.csv', encoding='cp949')
all_data = all_data.rename(columns={'back_hospital.sex':'sex', 'back_hospital.frequency':'frequency'})
all_data.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/모델링데이터/test.csv', index=False, encoding='cp949')
all_data

"""## 2016년도 검증 feature 통합"""

# 지역 합치기 & 정렬
# 학습데이터에서 사용한 코드 그대로 사용

def add_sort(file_names):
  total = pd.DataFrame()

  # 데이터 아래로 합치기
  for file_name in file_names:
    df = pd.read_csv(file_name, encoding='cp949')
    total = pd.concat([total,df])

  # ['일시','지점'] 우선순위로 정렬
  total=total.sort_values(by=[df.columns[1],df.columns[0]])

  # ['일시'] 컬럼 데이터 변경: 2012-01-01 -> 20120101
  # 데이터 타입 변경
  total['일시']= total['일시'].str.replace(pat=r'-',repl=r'',regex=True)
  total['일시'] = pd.to_numeric(total['일시'], errors='coerce')

  # 컬럼명 변경: '일시' -> 'yymmdd' / '지점' -> 'area'
  total = total.rename(columns={'일시':'yyyymmdd', '지점':'area'})

  return total

# 2016년 황사데이터 모든 지역 통합

years = ['2016']
total_C = pd.DataFrame()

for year in years:
  file_names = glob(f'/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/황사/지역별통합/{year}/*')
  total_C = pd.concat([total_C, add_sort(file_names)])

total_C.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/황사/2016.csv', index=False, encoding='cp949')

total_C

# 2016년 관측데이터 모든 지역 통합

years = ['2016']
total_C = pd.DataFrame()

for year in years:
  file_names = glob(f'/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/종관기상관측/지역별통합/{year}/*')

  total_C = pd.concat([total_C, add_sort(file_names)])

total_C.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/종관기상관측/2016.csv', index=False, encoding='cp949')

total_C

# 2016년 예보 데이터 확인
total_C = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/예보데이터/2016.csv', index_col=0, encoding='cp949')
total_C

# yyyymmdd 데이터 형태 바꾸기: 20120101 -> 2012-01-01
def convert(n):
  yy = n//10000
  mm = n%10000//100
  dd = n%100
  l = '{}-{:02d}-{:02d}'.format(yy,mm,dd)
  return l

# 백병원 + 황사 데이터

hospital = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/검증데이터/2-2_검증데이터셋.csv', encoding='cp949')

# yyyymmdd 데이터 형태 바꾸기
# 위에 만든 convert 함수 사용
total_C = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/황사/2016.csv', encoding='cp949')
total_C.info()
total_C['yyyymmdd'] = total_C['yyyymmdd'].apply(convert)

# 두 데이터 결합 후 'sex', 'frequency'컬럼 마지막 열로 보냄
total = pd.merge(hospital, total_C, on=["yyyymmdd", "area"], how="left")
total_final = total.drop(['sex', 'frequency'], axis=1, inplace=False)
total_final['sex'] = total['sex']
total_final['frequency'] = total['frequency']

total_final.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/검증데이터/hospital_feature.csv', index=False, encoding='cp949')

total_final

# [백병원 + 황사] + 관측

hospital = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/검증데이터/hospital_feature.csv', encoding='cp949')

total_C = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/종관기상관측/2016.csv', encoding='cp949')
total_C['yyyymmdd'] = total_C['yyyymmdd'].apply(convert)

total = pd.merge(hospital, total_C, on=["yyyymmdd", "area"], how="left")
total_final = total.drop(['sex', 'frequency'], axis=1, inplace=False)
total_final['sex'] = total['sex']
total_final['frequency'] = total['frequency']

total_final.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/검증데이터/hospital_feature1.csv', index=False, encoding='cp949')

total_final

# [백병원 + 황사 + 관측] + 예보
hospital = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/검증데이터/hospital_feature1.csv', encoding='cp949')

total_C = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/예보데이터/2016.csv', index_col=0, encoding='cp949')
total_C = total_C.rename(columns={'yymmdd':'yyyymmdd'})
total_C['yyyymmdd'] = total_C['yyyymmdd'].apply(convert)

total = pd.merge(hospital, total_C, on=["yyyymmdd", "area"], how="left")
total_final = total.drop(['sex', 'frequency'], axis=1, inplace=False)
total_final['sex'] = total['sex']
total_final['frequency'] = total['frequency']

total_final.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/검증데이터/hospital_feature2.csv', index=False, encoding='cp949')

total_final

"""## 2016년 대전, 세종 질병 발생 빈도수 평균값 구하기
- 학습데이터에 대전, 세종에 해당하는 학습할 수 있는 데이터가 없음
- 대전, 세종 지역은 질병 발생 빈도수 예측 불가능
- 각 년의 월일에 해당하는 질병 발생 빈도수의 평균값으로 대체
"""

hospital = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/back_hospital_.csv', index_col=0)
hospital.info()

# 문자열로 타입 변경 후 월,일에 해당하는 문자만 추출
hospital['back_hospital.yyyymmdd'] = hospital['back_hospital.yyyymmdd'].astype(str).str[4:]
hospital

# 날짜, 지역, 성별을 기준으로 빈도수 평균값 구하기
mean = pd.DataFrame(hospital.groupby(['back_hospital.yyyymmdd', 'back_hospital.area', 'back_hospital.sex'])['back_hospital.frequency'].mean())
mean.reset_index(inplace=True)  # 그룹화 풀기
mean

# 대전, 세종 지역만 추출
area = mean[(mean['back_hospital.area']=='대전')|(mean['back_hospital.area']=='세종')]

# 빈도수 반올림
area['back_hospital.frequency'] = area['back_hospital.frequency'].round()
area

# 날짜 컬럼의 데이터, 월일 앞에 년(2016) 추가
area['back_hospital.yyyymmdd'] = '2016' + area['back_hospital.yyyymmdd'].astype(str)
area

area.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/검증데이터/hospital_mean.csv', index=False, encoding='cp949')

"""## 종관기상관측 데이터 전처리"""

import pandas as pd
from glob import glob

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/종관기상관측/2012/강원/강원도_강릉_DAY_2012.csv', encoding='cp949')
df

# 필요한 데이터만 추출
df = df[['지점', '일시', '평균기온(°C)', '최저기온(°C)', '최고기온(°C)', '평균 풍속(m/s)', '평균 이슬점온도(°C)', '평균 상대습도(%)', '평균 증기압(hPa)', '1시간 최다일사량(MJ/m2)', '평균 지면온도(°C)']]
df

"""### 1. 이상치 제거
- 0.25 ~ 0.75 범위 사용
"""

df4 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/종관기상관측/2012/경남/경남_거제_DAY_2012.csv', encoding='cp949')
df4 = df4[['지점', '일시', '평균기온(°C)', '최저기온(°C)', '최고기온(°C)', '평균 풍속(m/s)', '평균 이슬점온도(°C)', '평균 상대습도(%)', '평균 증기압(hPa)', '평균 지면온도(°C)']] #, '1시간 최다일사량(MJ/m2)'

level_1q = df4.quantile(0.25)
print(level_1q,"\n")

level_3q = df4.quantile(0.75)
print(level_3q,"\n")
IQR = level_3q - level_1q

rev_range = 1.5  # 제거 범위 조절 변수
dff = df4[(df4 <= level_3q + (rev_range * IQR)) & (df4 >= level_1q - (rev_range * IQR))]
dff = dff.reset_index(drop=True)
dff[['지점', '일시']] = df4[['지점', '일시']]

dff

"""### 2. 결측치 처리"""

dff1 = dff.fillna(dff.interpolate()) # 가장 최근 위아래 평균으로 결측치 대입
dff1

dff1.isnull().sum()

"""### 3. 2012 ~ 2016년, 각 년도의 각 지역으로 통합
- 위의 내용들을 함수로 만들어 한번에 실행되도록 만듬
"""

def remove_add(area, year):
  # 이상치 제거 & 지역 합치기
  file_names = glob(f'/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/종관기상관측/{year}/{area}/*.csv') #폴더 내의 모든 csv파일 목록을 불러온다
  total = pd.DataFrame() #빈 데이터프레임 하나를 생성한다

  for file_name in file_names:
    df4 = pd.read_csv(file_name, encoding='cp949') #csv파일을 하나씩 열어 임시 데이터프레임으로 생성한다
    
    # 필요한 변수만 가져옴
    df4 = df4[['지점', '일시', '평균기온(°C)', '최저기온(°C)', '최고기온(°C)', '평균 풍속(m/s)', '평균 이슬점온도(°C)', '평균 상대습도(%)', '평균 증기압(hPa)', '평균 지면온도(°C)']] #, '1시간 최다일사량(MJ/m2)'

    # 이상치 처리(0.25~0.75)
    level_1q = df4.quantile(0.25)
    level_3q = df4.quantile(0.75)
    IQR = level_3q - level_1q

    rev_range = 1.5  # 제거 범위 조절 변수
    dff = df4[(df4 <= level_3q + (rev_range * IQR)) & (df4 >= level_1q - (rev_range * IQR))]
    dff = dff.reset_index(drop=True)
    dff[['지점', '일시']] = df4[['지점', '일시']]

    total = pd.concat([total, dff])

  
  # 결측치 처리
  total_N = total.fillna(total.interpolate()) # 가장 최근 위아래 평균으로 결측치 대입


  # ['일시'] 컬럼을 기준으로 그룹화 한 후 데이터 평균 구하기
  total_C = total_N.groupby(['일시']).mean()
  total_C = total_C.reset_index()
  total_C = total_C[['지점', '일시', '평균기온(°C)', '최저기온(°C)', '최고기온(°C)', '평균 풍속(m/s)', '평균 이슬점온도(°C)', '평균 상대습도(%)', '평균 증기압(hPa)', '평균 지면온도(°C)']] #, '1시간 최다일사량(MJ/m2)'
    
  total_C['지점'] = area

  return total_C

area_list = ['강원', '경기', '경남', '경북', '광주', '대구', '대전', '부산', '서울', '울산', '인천',
            '전남', '전북', '제주', '충남', '충북']
years = ['2012', '2013', '2014', '2015', '2016']

for year in years:
  for area in area_list:
    total_C = remove_add(area, "2015")
    total_C.to_csv(f'/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/종관기상관측/지역별통합/{year}/{area}_total.csv', index=False, encoding='cp949') # csv 파일로 저장

"""## 황사 데이터 전처리"""

file_names = glob('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/황사/2012/강원/*.csv')
total = pd.DataFrame()

# 강원 지역파일에 있는 모든 .csv파일 concat
for file_name in file_names:
  print(file_name)
  df = pd.read_csv(file_name, encoding='cp949')
  total = pd.concat([total,df])
total

# 각 데이터 프레임의 마지막에 다음해의 첫일이 있음
# 마지막행 => 2013-01-01 00:00

# '시간'컬럼에서 시간(01:00)에 해당하는 부분 제거

splited = total['시간'].str.split(' ', expand=True)
splited.columns = ['일시','시간']
new_df = pd.concat([total, splited], axis=1)
new_df.drop(['시간'], axis=1, inplace=True)
new_df

# 하루 단위로 평균값구하기

new_df1 = new_df.groupby(['일시']).mean()
new_df1 = new_df1.reset_index()
new_df1

# '지점'컬럼의 모든 데이터값을 '강원'으로 바꿈

new_df1['지점'] = '강원'
new_df1 = new_df1[['지점', '일시', '1시간평균 미세먼지농도(㎍/㎥)']]

new_df1 = new_df1.iloc[:-1,:]
new_df1

# 2012 ~ 2016년, 각 년도의 각 지역으로 통합
# 종관기상관측 데이터 전처리와 마찬가지로 위의 내용을 함수로 만들어 한번에 실행시킴

"""## 학습데이터 생성

### 1. 지역 합치기 & 정렬
"""

# 지역 합치기 & 정렬

def add_sort(file_names):
  total = pd.DataFrame()

  # 데이터 아래로 합치기
  for file_name in file_names:
    df = pd.read_csv(file_name, encoding='cp949')
    total = pd.concat([total,df])

  # ['일시','지점'] 우선순위로 정렬
  total=total.sort_values(by=[df.columns[1],df.columns[0]])

  # ['일시'] 컬럼 데이터 변경: 2012-01-01 -> 20120101
  # 데이터 타입 변경
  total['일시']= total['일시'].str.replace(pat=r'-',repl=r'',regex=True)
  total['일시'] = pd.to_numeric(total['일시'], errors='coerce')

  # 컬럼명 변경: '일시' -> 'yymmdd' / '지점' -> 'area'
  total = total.rename(columns={'일시':'yyyymmdd', '지점':'area'})

  return total

# 황사데이터 모든 지역 통합

years = ['2012', '2013', '2014', '2015', '2016']
total_C = pd.DataFrame()

for year in years:
  file_names = glob(f'/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/황사/지역별통합/{year}/*')
  total_C = pd.concat([total_C, add_sort(file_names)])

total_C.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/황사/total.csv', index=False, encoding='cp949')

total_C

# 관측데이터 모든 지역 통합

years = ['2012', '2013', '2014', '2015', '2016']
total_C = pd.DataFrame()

for year in years:
  file_names = glob(f'/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/종관기상관측/지역별통합/{year}/*')

  total_C = pd.concat([total_C, add_sort(file_names)])

total_C.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/종관기상관측/total.csv', index=False, encoding='cp949')

total_C

"""## 백병원과 변수 합치기"""

# 백병원 + 황사

hospital = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/back_hospital_.csv', index_col=0)

# hospital = hospital.drop(['Unnamed: 0_x', 'Unnamed: 0_y'], axis=1, inplace=False)
hospital.columns = ['yyyymmdd', 'area', hospital.columns[2], hospital.columns[3]]

total_C = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/황사/total.csv', encoding='cp949')

# 백병원데이터의 'yyyymmdd', 'area' 컬럼을 기준으로 황사 데이터와 결합
total = pd.merge(hospital, total_C, on=["yyyymmdd", "area"], how="left")
total_final = total.drop(['back_hospital.sex', 'back_hospital.frequency'], axis=1, inplace=False)
total_final['back_hospital.sex'] = total['back_hospital.sex']
total_final['back_hospital.frequency'] = total['back_hospital.frequency']

total_final.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/모델링데이터/hospital_feature.csv', index=False, encoding='cp949')

total_final

# [백병원 + 황사] + 관측
hospital = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/모델링데이터/hospital_feature.csv', encoding='cp949')

total_C = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/종관기상관측/total.csv', encoding='cp949')

# [백병원+황사]데이터의 'yyyymmdd', 'area' 컬럼을 기준으로 관측 데이터와 결합
total = pd.merge(hospital, total_C, on=["yyyymmdd", "area"], how="left")
total_final = total.drop(['back_hospital.sex', 'back_hospital.frequency'], axis=1, inplace=False)
total_final['back_hospital.sex'] = total['back_hospital.sex']
total_final['back_hospital.frequency'] = total['back_hospital.frequency']

total_final.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/모델링데이터/hospital_feature1.csv', index=False, encoding='cp949')

total_final

# [백병원 + 황사 + 관측] + 예보
hospital = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/모델링데이터/hospital_feature1.csv', encoding='cp949')

total_C = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/예보데이터/예보.csv', index_col=0, encoding='cp949')
total_C = total_C.drop(['Unnamed: 0.1'], axis=1, inplace=False)
total_C = total_C.rename(columns={'yymmdd':'yyyymmdd'})

# [백병원+황사+관측]데이터의 'yyyymmdd', 'area' 컬럼을 기준으로 예보 데이터와 결합
total = pd.merge(hospital, total_C, on=["yyyymmdd", "area"], how="left")
total_final = total.drop(['back_hospital.sex', 'back_hospital.frequency'], axis=1, inplace=False)
total_final['back_hospital.sex'] = total['back_hospital.sex']
total_final['back_hospital.frequency'] = total['back_hospital.frequency']

total_final.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/모델링데이터/hospital_feature2.csv', index=False, encoding='cp949')

# 검증데이터셋에 맞춰 컬럼명 변경
# 'back_hospital.sex' -> 'sex', 'back_hospital.frequency' -> 'frequency'

all_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/모델링데이터/hospital_feature2.csv', encoding='cp949')
all_data = all_data.rename(columns={'back_hospital.sex':'sex', 'back_hospital.frequency':'frequency'})
all_data.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/모델링데이터/test.csv', index=False, encoding='cp949')
all_data

"""## 2016년도 검증 feature 통합"""

# 지역 합치기 & 정렬
# 학습데이터에서 사용한 코드 그대로 사용

def add_sort(file_names):
  total = pd.DataFrame()

  # 데이터 아래로 합치기
  for file_name in file_names:
    df = pd.read_csv(file_name, encoding='cp949')
    total = pd.concat([total,df])

  # ['일시','지점'] 우선순위로 정렬
  total=total.sort_values(by=[df.columns[1],df.columns[0]])

  # ['일시'] 컬럼 데이터 변경: 2012-01-01 -> 20120101
  # 데이터 타입 변경
  total['일시']= total['일시'].str.replace(pat=r'-',repl=r'',regex=True)
  total['일시'] = pd.to_numeric(total['일시'], errors='coerce')

  # 컬럼명 변경: '일시' -> 'yymmdd' / '지점' -> 'area'
  total = total.rename(columns={'일시':'yyyymmdd', '지점':'area'})

  return total

# 2016년 황사데이터 모든 지역 통합

years = ['2016']
total_C = pd.DataFrame()

for year in years:
  file_names = glob(f'/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/황사/지역별통합/{year}/*')
  total_C = pd.concat([total_C, add_sort(file_names)])

total_C.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/황사/2016.csv', index=False, encoding='cp949')

total_C

# 2016년 관측데이터 모든 지역 통합

years = ['2016']
total_C = pd.DataFrame()

for year in years:
  file_names = glob(f'/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/종관기상관측/지역별통합/{year}/*')

  total_C = pd.concat([total_C, add_sort(file_names)])

total_C.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/종관기상관측/2016.csv', index=False, encoding='cp949')

total_C

# 2016년 예보 데이터 확인
total_C = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/예보데이터/2016.csv', index_col=0, encoding='cp949')
total_C

# yyyymmdd 데이터 형태 바꾸기: 20120101 -> 2012-01-01
def convert(n):
  yy = n//10000
  mm = n%10000//100
  dd = n%100
  l = '{}-{:02d}-{:02d}'.format(yy,mm,dd)
  return l

# 백병원 + 황사 데이터

hospital = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/검증데이터/2-2_검증데이터셋.csv', encoding='cp949')

# yyyymmdd 데이터 형태 바꾸기
# 위에 만든 convert 함수 사용
total_C = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/황사/2016.csv', encoding='cp949')
total_C.info()
total_C['yyyymmdd'] = total_C['yyyymmdd'].apply(convert)

# 두 데이터 결합 후 'sex', 'frequency'컬럼 마지막 열로 보냄
total = pd.merge(hospital, total_C, on=["yyyymmdd", "area"], how="left")
total_final = total.drop(['sex', 'frequency'], axis=1, inplace=False)
total_final['sex'] = total['sex']
total_final['frequency'] = total['frequency']

total_final.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/검증데이터/hospital_feature.csv', index=False, encoding='cp949')

total_final

# [백병원 + 황사] + 관측

hospital = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/검증데이터/hospital_feature.csv', encoding='cp949')

total_C = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/종관기상관측/2016.csv', encoding='cp949')
total_C['yyyymmdd'] = total_C['yyyymmdd'].apply(convert)

total = pd.merge(hospital, total_C, on=["yyyymmdd", "area"], how="left")
total_final = total.drop(['sex', 'frequency'], axis=1, inplace=False)
total_final['sex'] = total['sex']
total_final['frequency'] = total['frequency']

total_final.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/검증데이터/hospital_feature1.csv', index=False, encoding='cp949')

total_final

# [백병원 + 황사 + 관측] + 예보
hospital = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/검증데이터/hospital_feature1.csv', encoding='cp949')

total_C = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/예보데이터/2016.csv', index_col=0, encoding='cp949')
total_C = total_C.rename(columns={'yymmdd':'yyyymmdd'})
total_C['yyyymmdd'] = total_C['yyyymmdd'].apply(convert)

total = pd.merge(hospital, total_C, on=["yyyymmdd", "area"], how="left")
total_final = total.drop(['sex', 'frequency'], axis=1, inplace=False)
total_final['sex'] = total['sex']
total_final['frequency'] = total['frequency']

total_final.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/검증데이터/hospital_feature2.csv', index=False, encoding='cp949')

total_final

"""## 2016년 대전, 세종 질병 발생 빈도수 평균값 구하기
- 학습데이터에 대전, 세종에 해당하는 학습할 수 있는 데이터가 없음
- 대전, 세종 지역은 질병 발생 빈도수 예측 불가능
- 각 년의 월일에 해당하는 질병 발생 빈도수의 평균값으로 대체
"""

hospital = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/back_hospital_.csv', index_col=0)
hospital.info()

# 문자열로 타입 변경 후 월,일에 해당하는 문자만 추출
hospital['back_hospital.yyyymmdd'] = hospital['back_hospital.yyyymmdd'].astype(str).str[4:]
hospital

# 날짜, 지역, 성별을 기준으로 빈도수 평균값 구하기
mean = pd.DataFrame(hospital.groupby(['back_hospital.yyyymmdd', 'back_hospital.area', 'back_hospital.sex'])['back_hospital.frequency'].mean())
mean.reset_index(inplace=True)  # 그룹화 풀기
mean

# 대전, 세종 지역만 추출
area = mean[(mean['back_hospital.area']=='대전')|(mean['back_hospital.area']=='세종')]

# 빈도수 반올림
area['back_hospital.frequency'] = area['back_hospital.frequency'].round()
area

# 날짜 컬럼의 데이터, 월일 앞에 년(2016) 추가
area['back_hospital.yyyymmdd'] = '2016' + area['back_hospital.yyyymmdd'].astype(str)
area

area.to_csv('/content/drive/MyDrive/Colab Notebooks/기상청공모전/과제2/데이터/검증데이터/hospital_mean.csv', index=False, encoding='cp949')





import pandas as pd
import numpy as np

data = pd.read_csv('test.csv')
data.head()



"""## 회귀분석"""

data_drop = data.dropna(axis=0)
data_drop

import statsmodels.api as sm
X = data_drop.drop(['yyyymmdd','area','sex','frequency'], axis=1, inplace=False)
Y = data_drop['frequency']
res = sm.OLS(Y,X).fit()
print(res.summary())

X.corr()

"""#### 지면온도, 강수량 , 적설, 최저,최고빼고(일교차)남기고"""



"""### 2016년 결측치확인"""

yes = pd.read_csv('2016.csv')
yes.isna().sum()

yes2

yes2 = yes.sort_values(['area', 'yymmdd'])
yes3 = yes2[yes2['하늘상태'].isna()]
yes3.to_csv('하늘상태.csv', encoding='utf-8-sig')

"""### 충남 7/24 - 12/31, 충북1 - 12/31  하늘상태 결측치"""

data_2016 = pd.read_csv('검증데이터.csv')
data_2016

"""### 대전, 세종 제거"""

data_16 = data_2016[(data_2016['area']!='대전') &(data_2016['area']!='세종')]
data_16

data_16.isna().sum()

data_16 = data_16.sort_values(['area','yyyymmdd','sex'])
data_16

data_16_1 = data_16[(data_16['area']!='충남') & (data_16['area']!='충북')] # 충남,충북 제외
data_16_2 = data_16[(data_16['area']=='충남')] # 충남 
data_16_3 = data_16[(data_16['area']=='충북')] # 충북

"""### 충남 7/14~12/31 결측치채우기"""

data_16_2.isna().sum()

data_16_2

data = pd.read_csv('test.csv')
data.head()

data_2 = data[data['area']=='충남']
data_2 = data_2.reset_index()
data_2 = data_2.iloc[:,1:]
data_2['year'] = data_2['yyyymmdd']//10000
data_2['mmdd'] = data_2['yyyymmdd']%10000

nam_total = pd.DataFrame(data_2.groupby(['mmdd','sex'])['하늘상태'].mean())
nam_total = nam_total['하늘상태'].reset_index()
nam_total = nam_total['하늘상태']
nam_total.to_csv('충남_2.csv', encoding='utf-8-sig')

data_16_2 = data_16_2.reset_index()
data_16_2 = data_16_2.iloc[:,1:]
data_16_2.to_csv('충남.csv',encoding='utf-8-sig')

"""## excel에서 충남 7/14~12/31 결측치 4년평균으로 채우기"""

data_16_2 = pd.read_csv('충남.csv') # 채우고 파일 불러오기
data_16_2.isna().sum()

"""### 충북 1/1~12/31 결측치채우기"""

data_16_3.isna().sum()

data_3 = data[data['area']=='충북']
data_3 = data_3.reset_index()
data_3 = data_3.iloc[:,1:]
data_3['year'] = data_3['yyyymmdd']//10000
data_3['mmdd'] = data_3['yyyymmdd']%10000

buk_total = pd.DataFrame(data_3.groupby(['mmdd','sex'])['하늘상태'].mean())
buk_total = buk_total['하늘상태'].reset_index()
buk_total = buk_total['하늘상태']
buk_total.to_csv('충북_2.csv', encoding='utf-8-sig')

data_16_3 = data_16_3.reset_index()
data_16_3 = data_16_3.iloc[:,1:]
data_16_3.to_csv('충북.csv',encoding='utf-8-sig')

"""## excel에서 충북 1/1~12/31 결측치 4년평균으로 채우기"""

data_16_3 = pd.read_csv('충북.csv') # 채우고 파일 불러오기
data_16_3.isna().sum()

"""## 충남, 충북 결측치 처리 후 16년 data병합"""

data_16_total = pd.concat([data_16_1, data_16_2, data_16_3], axis=0)
data_16_total.isna().sum()
data_16_total = data_16_total.iloc[:,0:-1]
data_16_total.isna().sum()

"""## data중간에 있는 미세먼지농도 위아래 평균값으로 결측치 대체"""

data_16_total = data_16_total.interpolate()
data_16_total.isna().sum()

"""### 일교차 열 만들기"""

data_16_total['temp_diff'] = data_16_total['최고기온(°C)'] - data_16_total['최저기온(°C)']

"""### 지면온도, 강수량 , 적설, 최저,최고온도 제거"""

data_16_total = data_16_total.drop(['평균 지면온도(°C)','최고기온(°C)','최저기온(°C)', '강수량','적설'], axis=1, inplace=False)
data_16_total

def convert(n):
    yy = n.split('-')[0]
    mm = n.split('-')[1]
    dd = n.split('-')[2]
    return int(yy+mm+dd)

data_16_total['yyyymmdd'] = data_16_total['yyyymmdd'].apply(convert)

data_16_total

"""## 2012-2015 결측치제거"""

data = pd.read_csv('test.csv')
data_2 = data.dropna(axis=0)

"""### 일교차 열 만들기"""

data_2['temp_diff'] = data_2['최고기온(°C)'] - data_2['최저기온(°C)']

"""### 지면온도, 강수량 , 적설, 최저,최고온도 제거"""

data_2 = data_2.drop(['평균 지면온도(°C)','최고기온(°C)','최저기온(°C)', '강수량','적설'], axis=1, inplace=False)
data_2



"""### 2012 - 2015, 2016 data 통합"""

final = pd.concat([data_2, data_16_total], axis=0)
final.to_csv('통합원본.csv',encoding='utf-8-sig')

final



"""### 정규화

### 정규화 진행하지 않을 colums
"""

final1 = final.iloc[:,[0,1,10,11]]

"""### 정규화 진행할 columns"""

final2 = final.iloc[:,[2,3,4,5,6,7,8,9,12]]

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
final2[:] = scaler.fit_transform(final2[:])

final_re = pd.concat([final1,final2],axis=1)
final_re

final_re['year'] = final_re['yyyymmdd']//10000
final_re['mm'] = final_re['yyyymmdd']%10000//100

final_re

"""### dataset 나누기

- 학습데이터 : 2012~2015 ( 201203, 201306, 201409, 201512 제외)  
- 검증데이터 : 201203,201306,201409,201512  
- 테스트데이터 : 2016
"""

test_data = final_re[final_re['year']==2016]
test_data = test_data.drop(['year','mm'], axis=1, inplace=False)
test_data = test_data.sort_values(['yyyymmdd','sex','area'])
test_data = test_data.reset_index()
test_data = test_data.iloc[:,1:]
test_data.to_csv('test_data.csv',encoding='utf-8-sig')
test_data

vaild_data = final_re[((final_re['year']==2012) & (final_re['mm']==3)) | ((final_re['year']==2013) & (final_re['mm']==6)) |
                     ((final_re['year']==2014) & (final_re['mm']==9)) | ((final_re['year']==2015) & (final_re['mm']==12))]
vaild_data = vaild_data.drop(['year','mm'], axis=1, inplace=False)
vaild_data.to_csv('vaild_data.csv',encoding='utf-8-sig')

train_data = final_re[((final_re['year']==2012) & (final_re['mm']!=3)) | ((final_re['year']==2013) & (final_re['mm']!=6)) |
                     ((final_re['year']==2014) & (final_re['mm']!=9)) | ((final_re['year']==2015) & (final_re['mm']!=12))]
train_data = train_data.drop(['year','mm'], axis=1, inplace=False)
train_data.to_csv('train_data.csv',encoding='utf-8-sig')



"""### 모델링"""

from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

from sklearn.model_selection import GridSearchCV 
from sklearn.model_selection import TimeSeriesSplit 

from sklearn.metrics import mean_squared_error
from math import sqrt

import pandas as pd
import numpy as np

import warnings
warnings.filterwarnings('ignore')

"""### 전처리 x,y로 나누기"""

from google.colab import drive
drive.mount('/content/drive')

train = pd.read_csv('/content/drive/MyDrive/2차과제/train_data.csv', index_col=0, engine='python')  # 학습데이터
test = pd.read_csv('/content/drive/MyDrive/2차과제/vaild_data.csv', index_col=0, engine='python')  # 검증데이터 (201203, 201306, 201409, 201512)

# 더미변수
train = pd.get_dummies(train, columns=['area'])
test = pd.get_dummies(test, columns=['area'])

# 학습 데이터
trainX = train.drop(['frequency','yyyymmdd'], axis=1, inplace=False)
trainY = train['frequency']

# 검증 데이터
testX = test.drop(['frequency','yyyymmdd'], axis=1, inplace=False)
testY = test['frequency']

"""## 1. GradientBoostingRegressor"""

from sklearn.ensemble import GradientBoostingRegressor
cv = 4

paramGrid = {"subsample" : [0.5,0.7,0.9], 'n_estimators' : [5000], 'max_depth':[3], 'min_samples_leaf' : [5], 'min_samples_split':[2,4,6,8,10,12], 'learning_rate' : [0.1]}

model_GBR = GradientBoostingRegressor(random_state=42)


gridsearch_GBR = GridSearchCV(model_GBR, paramGrid, verbose=1,
                          cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]))

gridsearch_GBR.fit(trainX, trainY)

print(gridsearch_GBR)
print('gridsearch_RDFRCV 최고 평균 정확도 수치: {:.4f}'.format(gridsearch_GBR.best_score_))
print('gridsearch_RDFRCV 최적 하이퍼파라미터: ', gridsearch_GBR.best_params_)

scores_df = pd.DataFrame(gridsearch_GBR.cv_results_)
scores_df

from sklearn.metrics import mean_squared_error
from math import sqrt

best_model_GBR = gridsearch_GBR.best_estimator_
best_pred = best_model_GBR.predict(testX)

rmse = sqrt(mean_squared_error(testY, best_pred))

print("훈련 세트 정확도: {:.4f}".format(best_model_GBR.score(trainX, trainY)))
print("테스트 세트 정확도: {:.4f}".format(best_model_GBR.score(testX, testY)))
print('{}\n rmse: {:.4f}'.format(best_model_GBR, rmse))
gb_score = rmse

real_test = pd.read_csv('/content/drive/MyDrive/2차과제/test_data.csv', index_col=0, engine='python')  # 2016년도 data 불러오기

area = real_test.iloc[:, 1]
area = pd.DataFrame(area)
real_test = pd.get_dummies(real_test, columns=['area'])

real_testX = real_test.drop(['frequency','yyyymmdd'], axis=1, inplace=False) # frequency 열 제거

real_pred = best_model_GBR.predict(real_testX) # 최종 예측할 X 입력
print(real_pred)



"""### 2. XGboost"""

cv = 4

paramGrid = {"subsample" : [0.5, 0.7, 0.9], 'max_depth' : [3, 5, 7, 9,11],
             'n_estimators' : [20000], 'learning_rate' : [0.01, 0.1, 0.3]}

fit_params={"early_stopping_rounds":300, 
            "eval_metric" : "rmse", 
            "eval_set" : [[testX, testY]]}

model = XGBRegressor(random_state=42, tree_method='exact', gpu_id=0)

gridsearch = GridSearchCV(model, paramGrid, verbose=1,
                          cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]))

gridsearch.fit(trainX, trainY, **fit_params)

print(model)
print('GridSearchCV 최고 평균 정확도 수치: {:.4f}'.format(gridsearch.best_score_))
print('GridSearchCV 최적 하이퍼파라미터: ', gridsearch.best_params_)

scores_df = pd.DataFrame(gridsearch.cv_results_)
scores_df

from sklearn.metrics import mean_squared_error
from math import sqrt

best_model_XGBR = gridsearch.best_estimator_
best_pred = best_model_XGBR.predict(testX)

rmse = sqrt(mean_squared_error(testY, best_pred))

print("훈련 세트 정확도: {:.4f}".format(best_model_XGBR.score(trainX, trainY)))
print("테스트 세트 정확도: {:.4f}".format(best_model_XGBR.score(testX, testY)))
print('{}\n rmse: {:.4f}'.format(best_model_XGBR, rmse))
xgb_score = rmse

real_test = pd.read_csv('/content/drive/MyDrive/2차과제/test_data.csv', index_col=0, engine='python')  # 2016년도 data 불러오기

real_testX = real_test.drop(['frequency','yyyymmdd','area'], axis=1, inplace=False) # frequency 열 제거

real_pred = best_model_XGBR.predict(real_testX) # 최종 예측할 X 입력
print(real_pred)



"""### 3. LGBM"""

from lightgbm import LGBMRegressor
cv=4 

paramGrid = {"bagging_fraction" : [0.72], 'num_iterations' : [20000], 'max_depth':[7], 'min_data_in_leaf' : [8], 'learning_rate' : [0.03], 'colsample_bytree':[0.72],
              'scale_pos_weight': [1.5], 'lambda_l1':[0.1], 'lambda_l2':[0.35]}

fit_params={"early_stopping_rounds":300,
            "eval_metric" : "rmse", 
            "eval_set" : [[testX, testY]]}


model_LGBMR = LGBMRegressor(random_state=42)


gridsearch_LGBMR = GridSearchCV(model_LGBMR, paramGrid, verbose=1,
                          cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]))

gridsearch_LGBMR.fit(trainX, trainY, **fit_params)

print(gridsearch_LGBMR)
print('gridsearch_RDFRCV 최고 평균 정확도 수치: {:.4f}'.format(gridsearch_LGBMR.best_score_))
print('gridsearch_RDFRCV 최적 하이퍼파라미터: ', gridsearch_LGBMR.best_params_)

scores_df = pd.DataFrame(gridsearch_LGBMR.cv_results_)
scores_df

# 최적의 파라미터로 다시 모델링

best_model_LGBMR = gridsearch_LGBMR.best_estimator_
best_pred = best_model_LGBMR.predict(testX)
rmse = sqrt(mean_squared_error(testY, best_pred)) 

# print('{}\ 예측 정확도: {0:.4f} rmse: {0:.4f}'.format(model, accuracy, rmse))
print('{}\n rmse: {:.4f}'.format(best_model_LGBMR, rmse))
lgb_score = rmse

real_test = pd.read_csv('/content/drive/MyDrive/2차과제/test_data.csv', index_col=0, engine='python')  # 2016년도 data 불러오기

area = real_test.iloc[:, 1]
area = pd.DataFrame(area)
real_test = pd.get_dummies(real_test, columns=['area'])

real_testX = real_test.drop(['frequency','yyyymmdd'], axis=1, inplace=False) # frequency 열 제거

real_pred = best_model_LGBMR.predict(real_testX) # 최종 예측할 X 입력
print(real_pred)



"""### 3가지 앙상블"""

pred_gb = best_model_GBR.predict(real_testX)
pred_xgb = best_model_XGBR.predict(real_testX)
pred_lgb = best_model_LGBMR.predict(real_testX)

total_weight = (1. / gb_score) + (1. / xgb_score) + (1. / lgb_score)
real_pred = (pred_gb * (1. / gb_score) + pred_xgb * (1. / xgb_score) +  pred_lgb * (1. / lgb_score)) / total_weight

print(real_pred)
#총합이 1이 되게 모델마다 weight를 정함.
# train set 결과인 rmse의 역수에 비례해서 가중치 줌.
# 각 모델의 가중치와 예측값을 곱해서 최종 결과물 산출.



"""### 3가지 모델 & 앙상블 모델 여기서부터 동일"""

fre_df = pd.DataFrame(real_pred, columns=['frequency'])
fre_df.head()

## 예측한 빈도수와 년도, 성별 병합

real_testX = real_test.drop(['frequency'], axis=1, inplace=False) # frequency 열 제거

real_testX = real_testX.reset_index()
real_testX = pd.concat([area, real_testX], axis=1)

real_testX_2 = real_testX[['yyyymmdd','area','sex']]
real_testX_2

final = pd.concat([real_testX_2, fre_df],axis=1)

"""### 대전, 세종 예측 data 통합"""

da_se = pd.read_csv('/content/drive/MyDrive/2차과제/hospital_mean.csv', engine='python')

final_2 = pd.concat([final, da_se], axis=0)
final_2

"""### 검증양식으로 변경"""

final_2 = final_2.sort_values(['yyyymmdd','sex','area']) # 검증양식 순서를 위해 재정렬
fre = final_2['frequency']
fre = fre.reset_index()
fre = fre.iloc[:,1:]
test = pd.read_csv('/content/drive/MyDrive/2차과제/2-2_검증데이터셋.csv')

final = pd.concat([test,fre],axis=1)
final.to_csv('/content/drive/MyDrive/2차과제/pred.csv', encoding='utf-8-sig')



"""### 최고 성능의 모델"""

1) GradientBoosting {'subsample':0.7, 'n_estimators':1000, 'max_depth':3, 'min_samples_leaf':5, 'min_samples_split':2, 
                    'learning_rate':0.1}
2) XGboost {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators':20000,'subsample'
            0.7}  
3) LGMB (bagging_fraction=0.72, colsample_bytree=0.72, lambda_l1=0.1,
              lambda_l2=0.35, learning_rate=0.01, max_depth=7,
              min_data_in_leaf=8, num_iterations=20000, random_state=42,
              scale_pos_weight=1.5)

"""### 1,2,3번 최적 모델을 이용해 앙상블"""

pred_gb = best_model_GBR.predict(real_testX)
pred_xgb = best_model_XGBR.predict(real_testX)
pred_lgb = best_model_LGBMR.predict(real_testX)

total_weight = (1. / gb_score) + (1. / xgb_score) + (1. / lgb_score)
real_pred = (pred_gb * (1. / gb_score) + pred_xgb * (1. / xgb_score) +  pred_lgb * (1. / lgb_score)) / total_weight

print(real_pred)

"""### real_pred를 위에 방법처럼 검증양식으로 변경"""